Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz
100%|██████████| 170498071/170498071 [00:15<00:00, 10798480.47it/s]
Extracting ./data/cifar-10-python.tar.gz to ./data
Files already downloaded and verified

Net(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)

1,  2000| loss: 2.208
1,  4000| loss: 1.854
1,  6000| loss: 1.653
1,  8000| loss: 1.554
1, 10000| loss: 1.490
1, 12000| loss: 1.449
2,  2000| loss: 1.394
2,  4000| loss: 1.365
2,  6000| loss: 1.333
2,  8000| loss: 1.294
2, 10000| loss: 1.302
2, 12000| loss: 1.285
3,  2000| loss: 1.224
3,  4000| loss: 1.198
3,  6000| loss: 1.174
3,  8000| loss: 1.194
3, 10000| loss: 1.176
3, 12000| loss: 1.175
4,  2000| loss: 1.091
4,  4000| loss: 1.109
4,  6000| loss: 1.105
4,  8000| loss: 1.095
4, 10000| loss: 1.097
4, 12000| loss: 1.095
5,  2000| loss: 1.016
5,  4000| loss: 1.010
5,  6000| loss: 1.027
5,  8000| loss: 1.023
5, 10000| loss: 1.043
5, 12000| loss: 1.029
6,  2000| loss: 0.957
6,  4000| loss: 0.974
6,  6000| loss: 0.977
6,  8000| loss: 0.991
6, 10000| loss: 0.969
6, 12000| loss: 0.988
7,  2000| loss: 0.901
7,  4000| loss: 0.917
7,  6000| loss: 0.940
7,  8000| loss: 0.918
7, 10000| loss: 0.945
7, 12000| loss: 0.950
8,  2000| loss: 0.853
8,  4000| loss: 0.858
8,  6000| loss: 0.887
8,  8000| loss: 0.894
8, 10000| loss: 0.904
8, 12000| loss: 0.900
9,  2000| loss: 0.782
9,  4000| loss: 0.830
9,  6000| loss: 0.872
9,  8000| loss: 0.863
9, 10000| loss: 0.877
9, 12000| loss: 0.914
10,  2000| loss: 0.774
10,  4000| loss: 0.813
10,  6000| loss: 0.831
10,  8000| loss: 0.842
10, 10000| loss: 0.841
10, 12000| loss: 0.864
11,  2000| loss: 0.759
11,  4000| loss: 0.768
11,  6000| loss: 0.804
11,  8000| loss: 0.805
11, 10000| loss: 0.836
11, 12000| loss: 0.840
12,  2000| loss: 0.725
12,  4000| loss: 0.762
12,  6000| loss: 0.759
12,  8000| loss: 0.789
12, 10000| loss: 0.809
12, 12000| loss: 0.823
13,  2000| loss: 0.687
13,  4000| loss: 0.737
13,  6000| loss: 0.761
13,  8000| loss: 0.779
13, 10000| loss: 0.786
13, 12000| loss: 0.786
14,  2000| loss: 0.676
14,  4000| loss: 0.723
14,  6000| loss: 0.738
14,  8000| loss: 0.747
14, 10000| loss: 0.774
14, 12000| loss: 0.791
15,  2000| loss: 0.642
15,  4000| loss: 0.720
15,  6000| loss: 0.725
15,  8000| loss: 0.737
15, 10000| loss: 0.745
15, 12000| loss: 0.783
16,  2000| loss: 0.653
16,  4000| loss: 0.679
16,  6000| loss: 0.699
16,  8000| loss: 0.718
16, 10000| loss: 0.743
16, 12000| loss: 0.752
17,  2000| loss: 0.621
17,  4000| loss: 0.683
17,  6000| loss: 0.682
17,  8000| loss: 0.708
17, 10000| loss: 0.718
17, 12000| loss: 0.731
18,  2000| loss: 0.612
18,  4000| loss: 0.639
18,  6000| loss: 0.715
18,  8000| loss: 0.699
18, 10000| loss: 0.708
18, 12000| loss: 0.747
19,  2000| loss: 0.601
19,  4000| loss: 0.638
19,  6000| loss: 0.695
19,  8000| loss: 0.697
19, 10000| loss: 0.704
19, 12000| loss: 0.715
20,  2000| loss: 0.608
20,  4000| loss: 0.638
20,  6000| loss: 0.656
20,  8000| loss: 0.689
20, 10000| loss: 0.700
20, 12000| loss: 0.723

Accuracy: 61 %
